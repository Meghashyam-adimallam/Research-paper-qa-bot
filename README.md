# ğŸ“„ Research Papers Q&A Bot

An interactive chatbot built with Streamlit that allows you to **upload research papers (PDFs)** and ask questions in a **ChatGPT-style interface**. It uses **local LLMs via Ollama**, text embeddings, and **vector search with FAISS** to generate contextual, accurate answers from your documents.

---

## ğŸš€ Features

- ğŸ“ Upload **one or multiple** research papers (PDF)
- âœ‚ï¸ Automatic **text chunking and preprocessing**
- ğŸ§  **FAISS** vector store for fast similarity search
- ğŸ¤– Q&A using **Ollama LLMs** (e.g., `llama3`)
- ğŸ’¬ Chat-like interface with conversation history
- ğŸ–¼ï¸ Modern and responsive **Streamlit UI**
- ğŸ§¹ Clear chat and ğŸ’¾ download Q&A history
- ğŸ” `.env` excluded for secure key management

---

## ğŸ› ï¸ Project Structure

```
research-paper-qa-bot/
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ style.css           # UI styling
â”œâ”€â”€ qa_chain.py         # Builds the QA chain with LLM + retriever
â”œâ”€â”€ embed_store.py      # Embedding & vector DB creation
â”œâ”€â”€ ingest.py           # PDF loading & chunking
â”œâ”€â”€ utils.py            # Helper functions
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/Meghashyam-adimallam/Research-paper-qa-bot.git
cd Research-paper-qa-bot
```

### 2. Install Python packages

```bash
pip install -r requirements.txt
```

### 3. Install Ollama (for local LLM inference)

Install Ollama from [https://ollama.com](https://ollama.com) and run:

```bash
ollama run llama3
```

This ensures the `llama3` model is running at `http://localhost:11434`.

### 4. Set up your `.env` file

Create a `.env` file in the root directory with:

```
OLLAMA_BASE_URL=http://localhost:11434
```

> ğŸ” This is excluded from GitHub with `.gitignore` for security.

### 5. Launch the app

```bash
streamlit run app.py
```

---

## ğŸ’¡ How It Works

1. **Upload PDFs** â†’ Extracts and chunks text using PyMuPDF.
2. **Embeddings** â†’ Uses `all-MiniLM-L6-v2` to vectorize chunks.
3. **Vector Search** â†’ Uses FAISS to retrieve relevant content.
4. **Answering** â†’ Local LLM (e.g. `llama3` via Ollama) generates answers using retrieved context.
5. **UI** â†’ Modern, chat-style interface powered by Streamlit.

---

## ğŸ“ Example Use Cases

* Understand complex academic papers quickly
* Summarize key ideas from uploaded research
* Ask follow-up or deep-dive questions across multiple documents

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgements

* [Ollama](https://ollama.com/) for fast local LLM inference
* [LangChain](https://www.langchain.com/) for chaining embeddings + LLM
* [FAISS](https://github.com/facebookresearch/faiss) for vector search
* [Streamlit](https://streamlit.io/) for the beautiful UI

---

> Made with ğŸ’» and ğŸ” by Megha
